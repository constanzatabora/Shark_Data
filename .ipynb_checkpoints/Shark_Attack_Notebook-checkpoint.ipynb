{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb9c1997-e077-4f93-ac9f-86e44eccb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5468e70-5923-4418-8eda-f9ab1f195a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def import_data(url):\n",
    "    shark_df = pd.read_excel(\"https://www.sharkattackfile.net/spreadsheets/GSAF5.xls\")\n",
    "    shark_df.dropna(axis=1, how='all', inplace=True)\n",
    "    shark_df.dropna(axis=0, how='all', inplace=True)\n",
    "    return shark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc33da65-d6d9-4135-957b-9e314dd49f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Owen - edits to date cleaning\n",
    "def clean_dates2(df):\n",
    "    new_df = df.copy()\n",
    "    #6973 rows\n",
    "    #Removes \"reported\" from dates and trailing spaces\n",
    "    new_df['Date'] = new_df['Date'].str.replace('reported', '', case=False)\n",
    "    new_df[\"Date\"] = new_df[\"Date\"].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    new_df['Date'] = new_df['Date'].str.replace(' ', '-', regex=False)\n",
    "    new_df['Date'] = new_df['Date'].str.replace(r'-+', '-', regex=True)\n",
    "\n",
    "    #converts to date format\n",
    "    new_df['Date'] = pd.to_datetime(new_df['Date'], errors='coerce')\n",
    "\n",
    "    #drop rows invalid dates\n",
    "    new_df.dropna(subset=['Date'], inplace=True)\n",
    "    #5376 rows\n",
    "\n",
    "    #drop years before 1900\n",
    "    new_df = new_df[new_df[\"Year\"] >= 1900 ]\n",
    "    new_df = new_df[new_df['Date'] >= pd.Timestamp('1900-01-01')].reset_index(drop=True)\n",
    "    #5114 rows\n",
    "\n",
    "    return new_df\n",
    "#\n",
    "#test = clean_dates2(shark_df)\n",
    "#test[\"Year2\"] = test[\"Date\"].dt.year\n",
    "#test[test['Year'] != test['Year2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9083d17c-e2d9-4661-89fc-9399285c631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliska\n",
    "def clean_states(df: pd.DataFrame):\n",
    "\n",
    "    version_2 = shark_df.copy() #independent copy so we wont mess the potential DF\n",
    "    version_2 = version_2.dropna(subset=[\"State\"]) #get rid of empty values\n",
    "    version_2.loc[:, \"State\"] = version_2[\"State\"].str.lower() #convert to lowercase\n",
    "\n",
    "    #get rid of countries that occur 5 or less times\n",
    "    state_counts = version_2[\"State\"].value_counts()\n",
    "    threshold = 5\n",
    "    states_to_keep = state_counts[state_counts >= threshold].index\n",
    "    version_2 = version_2[version_2[\"State\"].isin(states_to_keep)]\n",
    "\n",
    "    #corrections\n",
    "    state_corrections = {\"westerm australia\": \"western australia\", \"western australia\" : \"western australia\",\n",
    "                        \"mirs bay \": \"mirs bay\", \"mirs bay\" : \"mirs bay\",\n",
    "                        \"baja california\" : \"california\",\n",
    "                        \" primorje-gorski kotar county\": \"primorje-gorski kotar county\",\n",
    "                        }\n",
    "    version_2[\"State\"] = version_2[\"State\"].replace(state_corrections) # apply corrections\n",
    "    version_2[\"State\"] = version_2[\"State\"].str.title() #get back the capital letter of each word in states\n",
    "\n",
    "    return version_2\n",
    "\n",
    "#version_2 = clean_states(version_1)\n",
    "#version_2[\"State\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47a164ac-1de2-473c-a73e-acbaf262833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Owen\n",
    "def clean_cols(df: pd.DataFrame):\n",
    "    \"\"\"Removes empty columns and named \"Fatal\" correctly \"\"\"\n",
    "    new_df = df.rename(columns={'Unnamed: 11': 'Fatal'})\n",
    "    new_df = new_df.drop(['href formula', 'href','Case Number', 'Case Number.1',\n",
    "       'original order', 'Unnamed: 21', 'Unnamed: 22', \"pdf\"], axis=1)\n",
    "    new_df = new_df.drop_duplicates()\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60137b88-d9e8-4884-a281-0f319505de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constanza\n",
    "\n",
    "def clean_type(df: pd.DataFrame):\n",
    "    \"\"\" Cleans \"type\" column \"\"\"\n",
    "    new_df = df.copy()\n",
    "    new_df['Type'] = new_df['Type'].replace({' Provoked': 'Provoked'})\n",
    "\n",
    "    values_to_replace = ['Questionable', 'Watercraft', 'Sea Disaster', '?', 'Unconfirmed', 'Unverified', 'Invalid', 'Under investigation', 'Boat']\n",
    "    new_df['Type'] = new_df['Type'].replace(values_to_replace, 'Unknown')\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae17d1d-5fe2-4136-b130-f7144bf8091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Owen\n",
    "def clean_country(df):\n",
    "    \"\"\"Tidies the \"Country\" column of the DataFrama \"\"\"\n",
    "    new_df = df.copy()\n",
    "    new_df = new_df.dropna(subset=[\"Country\"])\n",
    "\n",
    "    # Converts country column to consistent capitalisation and strips spaces\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].apply(lambda x: x.strip().title())\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].apply(lambda x: \"USA\" if x == \"Usa\" else x)\n",
    "\n",
    "    #Removes rows that contain Oceans and Seas for the country\n",
    "    new_df = new_df[~new_df[\"Country\"].str.contains(\"Ocean\", na=False)]\n",
    "    new_df = new_df[~new_df[\"Country\"].str.contains(\"Central Pacific\", na=False)]\n",
    "    new_df = new_df[~new_df[\"Country\"].str.contains(\" Sea\", na=False)]\n",
    "    new_df = new_df[~new_df[\"Country\"].str.contains(\"Persian Gulf\", na=False)]\n",
    "\n",
    "    #Corrects country names\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Ceylon (Sri Lanka)\", \"Sri Lanka\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Ceylon\", \"Sri Lanka\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Maldive Islands\", \"Maldives\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"St. Maartin\", \"St Martin\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"St. Martin\", \"St Martin\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Reunion Island\", \"Reunion\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Trinidad\", \"Trinidad & Tobago\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Tobago\", \"Trinidad & Tobago\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Turks And Caicos\", \"Turks & Caicos\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Sudan?\", \"Sudan\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"United Arab Emirates (Uae)?\", \"United Arab Emirates\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"United Arab Emirates (Uae)\", \"United Arab Emirates\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Western Samoa\", \"Samoa\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Scotland\", \"United Kingdom\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Crete\", \"Greece\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Okinawa\", \"Japan\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Columbia\", \"Colombia\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"England\", \"United Kingdom\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"New Britain\", \"Papua New Guinea\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"New Guinea\", \"Papua New Guinea\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace('St Helena, British Overseas Territory', \"St Helena\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace('Burma', \"Myanmar\")\n",
    "\n",
    "    #Counts occurences of each country\n",
    "    country_counts = new_df[\"Country\"].value_counts() #Contains 6923\n",
    "\n",
    "\n",
    "    # Filter countries that appear more than two times\n",
    "    countries_to_keep = country_counts[country_counts > 1].index\n",
    "    new_df = new_df[new_df[\"Country\"].isin(countries_to_keep)]\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110c8604-a034-409c-afca-6409dc3b7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Owen\n",
    "\n",
    "def hemisphere(df: pd.DataFrame):\n",
    "    \"\"\"Adds in a \"Hemisphere\" column using a dictionary to determine which hemisphere the country is in \"\"\"\n",
    "\n",
    "    new_df = df.copy()\n",
    "\n",
    "    hemi_dict = {\n",
    "        'American Samoa': \"South\",\n",
    "        'Antigua': 'North',\n",
    "        'Argentina': \"South\",\n",
    "        'Australia': \"South\",\n",
    "        'Azores': \"North\",\n",
    "        'Bahamas': \"North\",\n",
    "        'Barbados': \"North\",\n",
    "        'Belize': \"North\",\n",
    "        'Bermuda': \"North\",\n",
    "        'Brazil': \"Equator\",\n",
    "        'Myanmar': \"North\",\n",
    "        'Canada': \"North\",\n",
    "        'Cape Verde': \"North\",\n",
    "        'Cayman Islands': \"North\",\n",
    "        'Chile': \"South\",\n",
    "        'China': \"North\",\n",
    "        'Colombia': \"Equator\",\n",
    "        'Costa Rica': \"North\",\n",
    "        'Croatia': \"North\",\n",
    "        'Cuba': \"North\",\n",
    "        'Dominican Republic': \"North\",\n",
    "        'Ecuador': \"Equator\",\n",
    "        'Egypt': \"North\",\n",
    "        'El Salvador': \"North\",\n",
    "        'Fiji': \"South\",\n",
    "        'France': \"North\",\n",
    "        'French Polynesia': \"South\",\n",
    "        'Greece': \"North\",\n",
    "        'Grenada': \"North\",\n",
    "        'Guam': \"North\",\n",
    "        'Guinea': \"North\",\n",
    "        'Guyana': \"North\",\n",
    "        'Haiti': \"North\",\n",
    "        'Honduras': \"North\",\n",
    "        'Hong Kong': \"North\",\n",
    "        'Iceland': \"North\",\n",
    "        'India': \"North\",\n",
    "        'Indonesia': \"Equator\",\n",
    "        'Iran': \"North\",\n",
    "        'Iraq': \"North\",\n",
    "        'Ireland': \"North\",\n",
    "        'Israel': \"North\",\n",
    "        'Italy': \"North\",\n",
    "        'Jamaica': \"North\",\n",
    "        'Japan': \"North\",\n",
    "        'Johnston Island': \"North\",\n",
    "        'Kenya': \"Equator\",\n",
    "        'Kiribati': \"Equator\",\n",
    "        'Lebanon': \"North\",\n",
    "        'Liberia': \"North\",\n",
    "        'Libya': \"North\",\n",
    "        'Madagascar': \"South\",\n",
    "        'Malaysia': \"North\",\n",
    "        'Maldives': \"Equator\",\n",
    "        'Malta': \"North\",\n",
    "        'Marshall Islands': \"North\",\n",
    "        'Martinique': \"North\",\n",
    "        'Mauritius': \"South\",\n",
    "        'Mexico': \"North\",\n",
    "        'Micronesia': \"North\",\n",
    "        'Montenegro': \"North\",\n",
    "        'Mozambique': \"South\",\n",
    "        'Namibia': \"South\",\n",
    "        'New Caledonia': \"South\",\n",
    "        'New Zealand': \"South\",\n",
    "        'Nicaragua': \"North\",\n",
    "        'Nigeria': \"North\",\n",
    "        'Norway': \"North\",\n",
    "        'Palau': \"North\",\n",
    "        'Panama': \"North\",\n",
    "        'Papua New Guinea': \"South\",\n",
    "        'Peru': \"South\",\n",
    "        'Philippines': \"North\",\n",
    "        'Portugal': \"North\",\n",
    "        'Reunion': \"South\",\n",
    "        'Russia': \"North\",\n",
    "        'Samoa': \"South\",\n",
    "        'Saudi Arabia': \"North\",\n",
    "        'Senegal': \"North\",\n",
    "        'Seychelles': \"South\",\n",
    "        'Sierra Leone': \"North\",\n",
    "        'Singapore': \"North\",\n",
    "        'Solomon Islands': \"South\",\n",
    "        'Somalia': \"Equator\",\n",
    "        'South Africa': \"South\",\n",
    "        'South Korea': \"North\",\n",
    "        'Spain': \"North\",\n",
    "        'Sri Lanka': \"South\",\n",
    "        'St Helena, British Overseas Territory': \"South\",\n",
    "        'St Martin': \"North\",\n",
    "        'St Helena': \"South\",\n",
    "        'Sudan': \"North\",\n",
    "        'Taiwan': \"North\",\n",
    "        'Tanzania': \"Equator\",\n",
    "        'Thailand': \"North\",\n",
    "        'Tonga': \"South\",\n",
    "        'Trinidad & Tobago': \"North\",\n",
    "        'Tunisia': \"North\",\n",
    "        'Turkey': \"North\",\n",
    "        'Turks & Caicos': \"North\",\n",
    "        'USA': \"North\",\n",
    "        'United Arab Emirates': \"North\",\n",
    "        'United Kingdom': \"North\",\n",
    "        'Uruguay': \"South\",\n",
    "        'Vanuatu': \"South\",\n",
    "        'Venezuela': \"North\",\n",
    "        'Vietnam': \"North\",\n",
    "        'West Indies': \"North\",\n",
    "        'Yemen': \"North\"\n",
    "    }\n",
    "\n",
    "    #is assigning a \"Hemisphere\" column to new_df by mapping each entry in the \"Country\" column to a hemisphere based on a dictionary, hemi_dict.\n",
    "    new_df[\"Hemisphere\"] = new_df[\"Country\"].apply(lambda country: hemi_dict.get(country, \"Na\"))\n",
    "\n",
    "    #Code here\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bbc838d-7d39-4c16-84c8-0a6556e577ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filip\n",
    "def clean_sex(df):\n",
    "    df2 = df.copy()\n",
    "    df2[\"Sex\"] = df2[\"Sex\"].replace({ ' M': 'M', 'M ': 'M', 'M x 2': 'M',})\n",
    "    df2[\"Sex\"] = df2[\"Sex\"].replace(['.', 'lli', 'N'], np.nan)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf318a14-68ae-4cda-bf17-0895d58121bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filip\n",
    "\n",
    "def clean_age(shark_df):\n",
    "    shark_df[\"Age\"] = shark_df[\"Age\"].replace({\n",
    "        '30s': '30',\n",
    "        '20/30': '25',\n",
    "        '20s': '20',\n",
    "        '50s': '50',\n",
    "        '40s': '40',\n",
    "        '60s': '60',\n",
    "        \"20's\": '20',\n",
    "        '18 months': '2',\n",
    "        '18 or 20': '19',\n",
    "        '12 or 13': '13',\n",
    "        '8 or 10': '9',\n",
    "        '30 or 36': '33',\n",
    "        '6½': '6',\n",
    "        '21 & ?': '21',\n",
    "        '33 or 37': '35',\n",
    "        'mid-30s': '35',\n",
    "        '23 & 20': '21',\n",
    "        '28': '28',\n",
    "        '20?': '20',\n",
    "        \"60's\": '62',\n",
    "        '32 & 30': '31',\n",
    "        '16 to 18': '17',\n",
    "        'mid-20s': '25',\n",
    "        'Ca. 33': '33',\n",
    "        '45 ': '45',\n",
    "        '21 or 26': '24',\n",
    "        '20 ': '20',\n",
    "        '>50': '55',\n",
    "        '18 to 22': '20',\n",
    "        '9 & 12': '10',\n",
    "        '? & 19': '19',\n",
    "        '9 months': '1',\n",
    "        '25 to 35': '30',\n",
    "        '23 & 26': '24',\n",
    "        '33 & 37': '35',\n",
    "        '25 or 28': '26',\n",
    "        '30 & 32': '31',\n",
    "        '50 & 30': '40',\n",
    "        '13 or 18': '16',\n",
    "        '34 & 19': '31',\n",
    "        '33 & 26': '30',\n",
    "        '2 to 3 months': '1',\n",
    "        '43': '43',\n",
    "        '7 or 8': '8',\n",
    "        '17 & 16': '17',\n",
    "        'Both 11': '11',\n",
    "        '9 or 10': '10',\n",
    "        '36 & 23': '30',\n",
    "        '10 or 12': '11',\n",
    "        '31 or 33': '32',\n",
    "        '2½': '2',\n",
    "        '13 or 14': '14'\n",
    "    })\n",
    "\n",
    "    shark_df[\"Age\"] = shark_df[\"Age\"].str.strip()\n",
    "    shark_df[\"Age\"] = shark_df[\"Age\"].replace([\n",
    "        'Middle age', np.nan, '?',\n",
    "        '!2', 'teen', 'Teen', '!6', '!!', '45 and 15', '28 & 22',\n",
    "        '9 & 60', 'a minor', '28 & 26', '46 & 34', '28, 23 & 30', 'Teens',\n",
    "        '36 & 26', '\\xa0', ' ', '7      &    31',\n",
    "        'Elderly', 'adult', '(adult)',\n",
    "        '37, 67, 35, 27, ? & 27', '21, 34,24 & 35', '17 & 35',\n",
    "        'X', '\"middle-age\"', 'MAKE LINE GREEN', '\"young\"', 'F',\n",
    "        'young', '  ', 'A.M.',\n",
    "           '?    &   14', 'M', '',\n",
    "    ], np.nan)\n",
    "    return(shark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3856724f-b871-4dad-90cb-172a0a7cfd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "shark_df = import_data(\"https://www.sharkattackfile.net/spreadsheets/GSAF5.xls\")\n",
    "\n",
    "def cleaning(df):\n",
    "    df2 = df.copy()\n",
    "    df2 = clean_dates2(df2)\n",
    "    df2 = clean_country(df2)\n",
    "    df2 = hemisphere(df2)\n",
    "    df2 = clean_type(df2)\n",
    "    df2 = clean_states(df2)\n",
    "    df2 = clean_age(df2)\n",
    "    df2 = clean_sex(df2)\n",
    "    df2 = clean_cols(df2)\n",
    "    return df2\n",
    "\n",
    "df = cleaning(shark_df)\n",
    "\n",
    "shark_df = cleaning(shark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a81c8529-1f3e-4a58-9713-bbbc564bb4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Fatal</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-11 00:00:00</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Brevard County Orlando</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Teddy Witteman</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bite to left arm</td>\n",
       "      <td>N</td>\n",
       "      <td>?</td>\n",
       "      <td>Bull shark 6ft</td>\n",
       "      <td>Todd SmithFlorida today: News 4:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-07-18 00:00:00</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Western Australia</td>\n",
       "      <td>Trigg beach Sterling</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Ryan Lowther</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minor injury to lower left leg</td>\n",
       "      <td>N</td>\n",
       "      <td>1735hr</td>\n",
       "      <td>Undetermined small shall shark</td>\n",
       "      <td>Daily Mail: Sky News: The West Australian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-07-08 00:00:00</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Ponce de Leon Inlet Volusia County</td>\n",
       "      <td>Diving into Water</td>\n",
       "      <td>Dempsey Manhart</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lower left leg injury</td>\n",
       "      <td>N</td>\n",
       "      <td>11hr15</td>\n",
       "      <td>4-5ft Blacktip shark</td>\n",
       "      <td>Miami Herald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-07-05 00:00:00</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>New Smyrna Beach</td>\n",
       "      <td>Wading</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minor injury to left foot</td>\n",
       "      <td>N</td>\n",
       "      <td>16hr15</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>Sacbee Fox 35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-07-04 00:00:00</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Texas</td>\n",
       "      <td>South Padre Island</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Tabatha Sullivant</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bite to left leg calf muscle removed</td>\n",
       "      <td>N</td>\n",
       "      <td>?</td>\n",
       "      <td>Bull shark 6 ft</td>\n",
       "      <td>NBCDFW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Date    Year        Type    Country              State  \\\n",
       "1   2024-10-11 00:00:00  2024.0  Unprovoked        USA            Florida   \n",
       "7   2024-07-18 00:00:00  2024.0  Unprovoked  Australia  Western Australia   \n",
       "8   2024-07-08 00:00:00  2024.0  Unprovoked        USA            Florida   \n",
       "9   2024-07-05 00:00:00  2024.0  Unprovoked        USA            Florida   \n",
       "10  2024-07-04 00:00:00  2024.0  Unprovoked        USA              Texas   \n",
       "\n",
       "                              Location           Activity               Name  \\\n",
       "1               Brevard County Orlando            Surfing     Teddy Witteman   \n",
       "7                 Trigg beach Sterling            Surfing       Ryan Lowther   \n",
       "8   Ponce de Leon Inlet Volusia County  Diving into Water    Dempsey Manhart   \n",
       "9                     New Smyrna Beach             Wading         Not stated   \n",
       "10                  South Padre Island           Swimming  Tabatha Sullivant   \n",
       "\n",
       "   Sex  Age                                Injury Fatal    Time  \\\n",
       "1    M  NaN                      Bite to left arm     N       ?   \n",
       "7    M  NaN        Minor injury to lower left leg     N  1735hr   \n",
       "8    M  NaN                 Lower left leg injury     N  11hr15   \n",
       "9    M  NaN             Minor injury to left foot     N  16hr15   \n",
       "10   F  NaN  Bite to left leg calf muscle removed     N       ?   \n",
       "\n",
       "                          Species                                      Source  \n",
       "1                   Bull shark 6ft           Todd SmithFlorida today: News 4:  \n",
       "7   Undetermined small shall shark  Daily Mail: Sky News: The West Australian  \n",
       "8             4-5ft Blacktip shark                               Miami Herald  \n",
       "9                    Not specified                              Sacbee Fox 35  \n",
       "10                 Bull shark 6 ft                                     NBCDFW  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shark_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd6b92-01d5-4f4d-9fe4-5f5d7149b155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
